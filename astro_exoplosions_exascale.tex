\documentclass[11pt,twocolumn]{article}

\usepackage{times}

\usepackage[margin=1in]{geometry}

\usepackage[small,compact]{titlesec}                                            

\begin{document}

\begin{center}
{\sffamily \bfseries Modeling Astrophysical Explosions with Sustained Exascale Computing} \\
Michael Zingale\footnotemark[1], Alan Calder\footnotemark[1]
\end{center}
%
\footnotetext[1]{Stony Brook University}

Our understanding of stars and their fates is based on coupling
observations to theoretical models.  Unlike laboratory physicists, we
cannot perform experiments on stars, but rather must
patiently take what nature allows us to observe.  Simulation offers a means
of virtual experimentation, enabling a detailed understanding of the
most violent ongoing explosions in the Universe---the death of stars.
%% (e.g.\ tweaking the energy
%% generation or setting two stars on a collision course)

Stars can explode in a surprising variety of ways, driven by either
nuclear or gravitational potential energy release.  The
explosion can consume the entire star (or stars) or just
surface layers.  Exotic remnants, like neutron stars or black holes, or
simply nothing can be left behind.  Stellar explosions are critically
important sources of nucleosynthesis---for example, all of the iron in
the Universe results from these explosions, enriching the interstellar
medium in heavy elements. 

%% Astrophysics has been at the forefront of high-performance computing
%% for decades, with simulations of supernovae explosions being one of
%% the prime foci.  
Both DOE (including the national labs) and
NSF have supported the development and application of simulation codes
to stellar explosions.  Many successes have been met, but there are
still great uncertainties in the mechanisms of core collapse
supernovae (the death of massive stars) and thermonuclear supernovae
(the explosion of compact white dwarf stars).  Astronomy is also
entering the ``big data'' era, with survey telescopes like LSST coming
online toward the end of the decade that will greatly expand
observations of astrophysical transients, perhaps finding entire new
classes to be understood.

Cutting edge research in stellar astrophysics is being done with both
large multidimensional simulations, demanding 10s of millions of
core-hours, as well as suites of one-dimensional evolutionary
simulations with exceptionally detailed microphysics.  The interplay
between these two paths is critical to building up a physical picture
of stellar explosions, and each has unique (and increasing)
computational demands.  For brevity, we focus here on the needs for
multi-dimensional work.

The standard practice in stellar astrophysics is to describe the star
as a fluid and use domain decomposition to divide the work across
computational nodes.  Ideally, shared memory parallelism is used
within a node, reducing memory overheads, and MPI is used across
nodes.  Work on exploiting accelerators (GPUs and Phi processors) is
underway for many codes, and standard technologies (MPI, OpenMP, and
OpenACC) promise to allow for portability.

An example of a success in our field is enormous progress made over
the last decades in understanding thermonuclear (type Ia) supernovae.
Core models were developed in the 1990s through one-dimensional
calculations, raising a host of complex questions about the physical
processes in the stars.  Large-scale multi-d simulations followed
these and explored a variety of progenitor systems and explosion
mechanisms, allowing researcher to address not just the question of a
successful explosion, but deeper issues such as systematic effects on
the brightness of an event and explanations for unusual or outlying
events.  Recently three-dimensional progenitor models (of both types
of supernovae) became feasible, greatly increasing our understanding
of the initial conditions to the explosion.  These problems require
the interaction of many different researchers and simulation codes
capable of modeling the different phases.  

\paragraph*{Increasing computing power.}

An increase of 100$\times$ in computing power will allow for 
simulations at unprecedented fidelity. Fluid flows are chaotic, and a
range of instabilities and turbulence are ever present in our models
of exploding stars. It was only recently that simulations switched
from being predominantly two-dimensional to
three-dimensional---enabled by the large increasing in computing power
over the last decade.  The goal of extant simulations is to understand
the feasibility of different theoretical models for explosions and to
probe the physics of the explosion mechanism itself.  However, the
range of length and timescales in stars that can be captured through
simulation is a small fraction of the true scales in stars.  This
means that approximations are made either explicitly (through the
introduction of subgrid scale models) or implicitly (having a
numerical dissipation that operates on much large scales than nature
would have).  Convergence studies (changing the resolution and hoping
for the same qualitative behavior) can test our assumptions, but
looming question is whether there is some resolution, yet
unobtainable, where the nature of the solution will change
qualitatively.  The promised increase in computing power will allow
for a much greater range in length scales to be modeled.

The increase in computer time also will allow for an expansion of the
physics modeled.  Today, small nuclear reaction networks ($\sim$
10--20 nuclei) are used, which approximately capture the energetics of
the flow, but are unable to make detailed predictions of the
nucleosynthetic yields.  Additional pieces, like radiation transport
are crudely approximated (if modeled at all).  The increase in
computational power will allow for more realistic physics throughout.

Finally, such an increase in computing time would bring today's
leadership computing into the realm of routine, allowing investigators
to perform many times the present volume of production simulations.
In astrophysics, this increase will allow suites of simulations 
investigate the sensitivity of events on both model and physical
parameters, enabling formal uncertainty quantification at 
unprecedented levels.  This is true of both the multi-dimensional
and one-dimensional simulations.

\paragraph*{Impacts.} In addition to answering questions about the 
astrophysics, research into modeling astrophysical explosions at the
exascale will significantly impact many fields of science that address
multi-scale, multi-physics applications.  The simulation codes
developed for these problems have application to terrestial combustion
phenomena, climate and atmospherical flows, DOE laboratory interests.
Importantly, this research provides an excellent training ground for
the next generation of computational scientistis, who can take their
training to other disiplinces (and industry).


\paragraph*{Capabilities needed.}  Computational fluid dynamics requires 
high performance interconnects, as nearest neighbor (and global for
some physics) communication is needed each timestep.  This means that
traditional supercomputers are required over simple clusters.  A major
challenge with the increase in simulation size is the analysis of the
data (100s of TB per simulation).  In situ data analysis will need to
take on a bigger role in the future---many of the pieces for this are
coming into place.

Supercomputing centers often favor ``hero'' calculations---those that
use a significant fraction of the machine.
However, science often needs capacity computing---many parameter
studies of the system help us understand the robustness of our models.
Going forward, there is a need for both capacity and leadership-class
computing centers.

Finally, there is an increasing desire to share simulation results,
which may be the seeds for follow-on simulations.  This
requires guaranteed long-term storage accessible to the community as a
whole.

\paragraph*{Foundational issues.}

The progress in our field is driving not just by increased FLOPS, but
also through innovative new algorithms.  Developing, maintaining, and
supporting simulation codes takes considerable effort, crossing
interdisciplinary lines (with coordination between domain scientists,
mathematicians, and computational scientists).  Funding mechanisms
need to recognize this interdisciplinary nature of computation.  A
further issue is that often code development work is not given the
same recognition and rewards as the scientific results themselves.
This puts the code developers, especially those early in their
careers, at a competitive disadvantage.  Perceptions of the role of
code work will need to adapt.  Finally, increased support for code
development and community support through the traditional grant
process would greatly help to capitalize on code investments.  Open
source codes also greatly help amortize the costs of code development,
and enable (and encourage) reproducibility of results, a hallmark of
science.  The astrophysics community does a reasonable job in making
codes available, and incentive structures should be setup to further
encourage this.

Another complication is that awards of computer time don't come with
monetary support for the researchers who will run and analyze the
simulations, and grants don't come with a guarantee of computer time,
so there is a chicken-and-egg problem, and a necessity of having to
have both in place independently to make the best use of either
resource.

Finally, continued support for training of students is essential (the
Argonne Training Program on Extreme-Scale Computing is an excellent
example).



\end{document}
